# Теория машинного обучения #
## Метрические алгоритмы классификации ##
Метрические алгоритмы классификации - алгоритмы, основанные на вычислении оценок сходства между объектами. Одним из вариантов сходства является расстояние между объектами. Тогда вводится функция расстояния ![](http://latex.codecogs.com/gif.latex?%24%5Crho%20%28x_1%2C%20x_2%29%24).
### Метод $k$ ближайших соседей ($kNN$) ###
Один из самых простых метрических алгоритмов классификации. Работает следующим образом: пусть дан классифицируемый объект $z$ и обучающая выборка $X^l$. Требуется определить класс объекта $z$ на основе данных из обучающей выборки. Для этого:
1. Вся выборка $X^l$ сортируется по возрастанию расстояния от объекта $z$ до каждого объекта выборки.
2. Проверяются классы $k$ ближайших соседей объекта $z$. Класс, встречаемый наиболее часто, присваивается объекту $z$.  

Вход алгоритма - классифицируемый объект, обучающая выборка и параметр $k$ - число рассматриваемых ближайших соседей.
Выход алгоритма - класс классифицируемого объекта.

При $k=1$ алгоритм превращается в $1NN$, то есть объекту $z$ присваивается класс его первого ближайшего соседа, все остальные объекты выборки не учитываются.

Оптимальное значение $k$ подбирается по Критерию Скользящего Контроля ($LOO$). Суть критерия: пусть дана обучающая выборка $X^l$. Требуется определить оптимальное значение параметра $k$ для данной выборки. Для этого:
1. Из выборки удаляется $i$-й объект $x^i$.
2. Выбранный алгоритм классификации запускается для $x^i$ и оставшейся выборки. По окончании работы алгоритма полученный класс объекта $x^i$ сравнивается с его реальным классом. При их несовпадении сумма ошибки увеличивается на $1$.
3. Шаги 1 и 2 повторяются для каждого объекта выборки при фиксированном $k$. По окончании работы алгоритма полученная сумма ошибки $sum$ делится на размер выборки: $sum=\frac{sum}{l}$.  Потом значение $k$ меняется, и алгоритм повторяется для нового значения. $k$ с наименьшим значением суммы ошибки будет оптимальным.
#### Реализация
При реализации алгоритма, в качестве обучающей выборки использовалась выборка ирисов Фишера. В качестве признаков объектов использовались значения длины и ширины лепестка. Значение $k$ подбиралось по $LOO$.

Алгоритм:

    kNN <- function(xl, z, k) {
	  orderedXL <- sortObj(xl, z);
	  n <- dim(orderedXL)[2]
	  classes <- orderedXL[1:k, n] 
	  counts <- table(classes) # Таблица встречаемости каждого класса среди k ближайших соседей объекта
	  class <- names(which.max(counts)) # Наиболее часто встречаемый класс
	  return (class)
	}
где $orderedXL$ - отсортированная обучающая выборка.

![LOO_kNN.png](https://github.com/IHappyPlant/RProjects/blob/master/img/LOO_kNN.png) ![kNN.png](https://github.com/IHappyPlant/RProjects/blob/master/img/kNN.png) 

Достоинства алгоритма:
1. Простота реализации
2. Хорошее качество, при правильно подобранной метрике и параметре $k$

Недостатки алгоритма:
1. Необходимость хранить выборку целиком
2. Малый набор параметров
3. Качество классификации сильно зависит от выбранной метрики

### Метод $k$ взвешенных ближайших соседей ($kwNN$)
